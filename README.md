# ğŸŒ QuakeWatch - Earthquake Monitoring Dashboard

A Flask-based web application that displays real-time and historical earthquake data from the USGS API, containerized with Docker and deployed to Kubernetes using Helm.

## ğŸš€ Features

- ğŸ“Š Real-time earthquake data visualization
- ğŸ—ºï¸ Interactive maps and charts using Matplotlib
- ğŸ³ Dockerized Flask Application
- â˜¸ï¸ Deployed on Kubernetes with k3s
- ğŸ”„ GitOps deployment with ArgoCD
- ğŸ“ˆ Horizontal Pod Autoscaler (HPA) for scaling
- ğŸ¤– Automated CI/CD with GitHub Actions
- ğŸ“ Structured logging to `/var/log/flask-data`
- ğŸ“Š Health monitoring endpoints
- ğŸ” Prometheus metrics collection & Grafana dashboards

---

## ğŸ§° Prerequisites

- [k3s](https://k3s.io/) - Lightweight Kubernetes distribution
- [kubectl](https://kubernetes.io/docs/tasks/tools/)
- [Helm](https://helm.sh/docs/intro/install/)
- Internet connection (for pulling Docker images from Docker Hub)

---

### ğŸªŸ For Windows Users (WSL2)

1. Install and enable **WSL 2** (if not already):
   - [WSL 2 installation guide](https://learn.microsoft.com/en-us/windows/wsl/install)
2. Install Ubuntu or your preferred Linux distribution from Microsoft Store
3. Open WSL terminal and proceed with k3s installation (handled by deployment script)

---

### ğŸ§ For Linux Users

k3s will be automatically installed by the deployment script if not already present.

**Optional - Install Google Chrome for automatic browser opening:**
```bash
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
sudo apt update
sudo apt install google-chrome-stable -y
```

## â–¶ï¸ Quick Deployment

```bash
./build-deploy.sh
```

This automated script will:
1. Install k3s if not already present
2. Deploy Prometheus & Grafana monitoring stack
3. Deploy ArgoCD for GitOps continuous deployment
4. Apply all Kubernetes resources via Helm
5. Set up port-forwarding to http://localhost:8080
6. Display ArgoCD and Grafana credentials for UI access

---

## ğŸ“¦ Application Components

### API Endpoints
- `/` - Main dashboard page
- `/graph-earthquakes` - Interactive graphs and data
- `/graph-earthquakes.png` - Dynamic graph image generation
- `/telaviv-earthquakes` - Regional earthquake data
- `/ping`, `/health`, `/status`, `/info` - Health check endpoints

### Kubernetes Resources
- **Deployment**: 3 replicas with resource limits and health checks
- **Service**: NodePort service on port 32000
- **ConfigMap**: Application configuration
- **Secret**: API keys and sensitive data
- **PVC**: Persistent storage for logs (1Gi)
- **HPA**: Auto-scaling (2-5 replicas based on CPU usage)
- **CronJob**: Automated logging every minute

---


## â–¶ï¸ Accessing the Application

After deployment completes, the script automatically sets up port-forwarding and displays:

```
âœ… Service available at: http://localhost:8080
â„¹ï¸  Port-forward PID: <process-id> (to stop: kill <process-id>)
```

The application will automatically open in Google Chrome if installed, or you can manually navigate to `http://localhost:8080`

### Stopping the Port-Forward

The port-forward runs in the background. To stop it:
```bash
kill <process-id>  # Use the PID shown in deployment output
```

Or kill all port-forwards:
```bash
pkill -f "port-forward.*earthquake-app"
```

---

## ğŸ” ArgoCD Access

The deployment uses **ArgoCD** for GitOps-based continuous deployment. ArgoCD automatically monitors the Git repository and keeps your cluster in sync with the desired state.

### Accessing ArgoCD UI

After deployment, the script displays ArgoCD credentials:

```
ğŸ” ArgoCD Credentials:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Username: admin
Password: <auto-generated-password>
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**To access the ArgoCD UI:**

1. Start port-forward to ArgoCD server:
   ```bash
   kubectl port-forward svc/argocd-server -n argocd 8081:443
   ```

2. Open your browser and navigate to:
   ```
   https://localhost:8081
   ```

3. Login with the credentials displayed during deployment

### What ArgoCD Does

- **GitOps Deployment**: Automatically deploys changes pushed to the `main` branch
- **Auto-Sync**: Keeps cluster state synchronized with Git repository
- **Self-Healing**: Automatically corrects manual changes to match Git state
- **Rollback**: Easy rollback through Git history

The ArgoCD Application manifest is located at `argocd/argocd.yaml` and monitors the `quackwatch-helm/` directory.

### Sync Waves - Deployment Ordering

The deployment uses **ArgoCD Sync Waves** to ensure resources are created in the correct order:

**Wave 0 - Infrastructure** (deployed first):
- Secret (earthquake-secret)
- PersistentVolumeClaim (logs storage)

**Wave 1 - Configuration**:
- ConfigMap (application config)
- ServiceAccount (permissions)

**Wave 2 - Application** (deployed last):
- Deployment (earthquake app pods)
- Service (networking)
- HorizontalPodAutoscaler (auto-scaling)
- CronJob (date logger)

This ensures that:
1. Secrets and storage are available before the app needs them
2. Configuration is loaded before pods start
3. The app only starts when all dependencies are ready

You can view sync waves in ArgoCD UI by checking the "Sync Wave" annotation on each resource.

---

## ğŸ”§ Development

### Local Development (Flask only)
```bash
cd QuakeWatch
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python app.py
```
Application runs on http://127.0.0.1:5000

### Docker Development
```bash
docker-compose up --build
```
Application runs on http://localhost:8000

### Testing
```bash
# Install test dependencies
pip install -r requirements-test.txt

# Run Helm deployment tests
python test_helm_deployment.py --release-name quackwatch-test --namespace default

# Run with pytest
pytest test_helm_deployment.py::TestHelmDeployment -v
```

---

## ğŸ¤– CI/CD Pipeline

The project uses GitHub Actions for automated testing across multiple Kubernetes versions:

### Workflow Features
- **Multi-version testing**: Tests against Kubernetes 1.27, 1.28, and 1.29
- **Automated deployment**: Deploys Helm chart to Minikube
- **Comprehensive testing**: Health checks, scaling, and functionality tests
- **Parallel execution**: Matrix strategy for faster testing
- **Auto cleanup**: Resources cleaned up after tests

### Trigger Events
- Push to `main`, `develop`, or `feature/*` branches
- Pull requests to `main` or `develop`
- Manual workflow dispatch

---

## ğŸ“Š Monitoring & Observability

### Prometheus & Grafana Stack

The deployment includes a complete monitoring stack with Prometheus and Grafana for real-time metrics and visualization.

**Automatically installed components:**
- **Prometheus**: Metrics collection and storage
- **Grafana**: Metrics visualization and dashboards
- **ServiceMonitor**: Automatic metrics scraping from QuakeWatch app
- **Pre-configured Dashboard**: QuakeWatch application metrics

### Accessing Grafana

After deployment, Grafana credentials are displayed. To access the Grafana UI:

```bash
kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80
```

Then navigate to: **http://localhost:3000**

**Default Credentials:**
- Username: `admin`
- Password: (displayed during deployment or retrieve with command below)

```bash
kubectl get secret kube-prometheus-stack-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d && echo
```

### QuakeWatch Dashboard

The deployment includes a pre-configured dashboard showing:
- **Request Rate**: HTTP requests per second
- **Response Time**: p95 latency metrics
- **Error Rate**: 4xx and 5xx errors
- **Total Requests**: Cumulative request counter

Find it in Grafana: **Dashboards â†’ QuakeWatch Application Metrics**

### Prometheus Metrics

The Flask application exposes Prometheus metrics at `/metrics` endpoint:

```bash
# View raw metrics
curl http://your-service:5000/metrics
```

**Available metrics:**
- `flask_http_request_total` - Total HTTP requests by method and status
- `flask_http_request_duration_seconds` - Request duration histogram
- `flask_http_request_duration_seconds_sum` - Total request duration
- `app_info` - Application metadata

### Accessing Prometheus UI

```bash
kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090
```

Navigate to: **http://localhost:9090**

### Health Checks
The application includes comprehensive health monitoring:
```bash
curl http://your-service/health
curl http://your-service/ping
curl http://your-service/status
```

### Horizontal Pod Autoscaler
Automatically scales between 2-5 replicas based on CPU usage:
```bash
kubectl get hpa quackwatch-helm
```

### View Logs
```bash
kubectl logs -l app.kubernetes.io/instance=quackwatch-helm -f
```

---

## ğŸ³ Container Registry

**Docker Hub**: `blaqr/earthquake:latest`
```bash
docker pull blaqr/earthquake:latest
```

**Helm Chart**: [Available in this repository at](https://github.com/users/gabrielrosinski/packages/container/package/quackwatch-helm)

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ QuakeWatch/                 # Flask application source
â”‚   â”œâ”€â”€ app.py                 # Application factory with Prometheus metrics
â”‚   â”œâ”€â”€ dashboard.py           # Main dashboard blueprint
â”‚   â”œâ”€â”€ utils.py               # Helper functions
â”‚   â”œâ”€â”€ templates/             # Jinja2 templates
â”‚   â””â”€â”€ static/                # Static assets
â”œâ”€â”€ quackwatch-helm/           # Helm chart
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â””â”€â”€ templates/             # Kubernetes manifests
â”‚       â”œâ”€â”€ deployment.yaml
â”‚       â”œâ”€â”€ service.yaml
â”‚       â”œâ”€â”€ servicemonitor.yaml      # Prometheus metrics scraping
â”‚       â”œâ”€â”€ grafana-dashboard.yaml   # Pre-configured Grafana dashboard
â”‚       â””â”€â”€ ...
â”œâ”€â”€ argocd/                    # ArgoCD configuration
â”‚   â””â”€â”€ argocd.yaml            # Application manifest
â”œâ”€â”€ test_helm_deployment.py    # Deployment tests
â”œâ”€â”€ .github/workflows/ci.yml   # CI/CD pipeline
â”œâ”€â”€ build-deploy.sh            # Automated deployment script
â””â”€â”€ docker-compose.yml         # Local development
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `pytest test_helm_deployment.py -v`
5. Submit a pull request

The CI pipeline will automatically test your changes across multiple Kubernetes versions.

---

## ğŸ“„ License

This project is part of a DevOps course demonstrating modern containerization and orchestration practices.